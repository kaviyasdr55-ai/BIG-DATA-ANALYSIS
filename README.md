# BIG-DATA-ANALYSIS

COMPANY : CODTECH IT SOLUTIONS

NAME : KAVIYA THARANI

INTERN ID : CTIS0821

DOMAIN : DATA ANALYSIST

DURATION : 4 WEEKS

MENTOR : NEELA SANTHOSH


## Big Data Analysis using PySpark

Big Data analysis refers to the process of examining large volumes of data to discover patterns, trends, and useful insights.
Traditional data processing tools are not efficient when the data size becomes very large.
To overcome this limitation, big data frameworks like Apache Spark are used.

In this project, PySpark (Python API for Apache Spark) is used to demonstrate big data scalability.
A large dataset containing nearly one million records was created and processed on a local machine.
Various operations such as data creation, transformation, aggregation, and visualization were performed.

The analysis includes grouping data into categories and counting the number of records in each group.
The results show that PySpark…

## Abstract

Big Data refers to extremely large datasets that cannot be processed efficiently using traditional data processing techniques.
This project focuses on analyzing large-scale data using PySpark, the Python interface for Apache Spark.
A dataset containing nearly one million records was created and processed on a local system.
Various data operations such as transformation, aggregation, and visualization were performed.
The project demonstrates how PySpark can efficiently handle big data and provide meaningful insights.
## Problem Statement

Traditional data processing tools struggle to handle large datasets efficiently due to memory and performance limitations.
There is a need for scalable and efficient tools that can process large volumes of data quickly.
The objective of this project is to use PySpark to analyze a large dataset and demonstrate its ability to handle big data efficiently on a local machine.

## Big Data Analysis Description

Big Data analysis involves processing and analyzing large datasets to extract useful information.
In this project, PySpark is used to create and analyze a dataset with nearly one million rows.
The data is processed using Spark DataFrame operations such as grouping and counting.
These operations demonstrate Spark’s ability to perform distributed data processing efficiently.

## Results and Insights

The dataset was successfully processed using PySpark without performance issues.
The groupBy and count operations showed that the data was evenly distributed across different categories.
The execution time was significantly faster compared to traditional processing methods.
This proves the scalability and efficiency of PySpark for big data analysis.

## Conclusion

This project successfully demonstrates big data analysis using PySpark.
Large datasets can be processed efficiently using Spark even on a local system.
PySpark provides scalability, high performance, and ease of use for big data applications.
This project highlights the importance of big data tools in modern data analytics.


<img width="1646" height="902" alt="Image" src="https://github.com/user-attachments/assets/77ad86de-6c5f-4841-b061-9d8834a04146" />
<img width="1475" height="788" alt="Image" src="https://github.com/user-attachments/assets/20128b0d-6be7-44f1-bb61-941b6814c451" />
<img width="1522" height="742" alt="Image" src="https://github.com/user-attachments/assets/1750d977-d536-47f5-a32d-41c4b65d28c9" />
